{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6484,"status":"ok","timestamp":1704364289892,"user":{"displayName":"RASIL RISAM F 21CSR163","userId":"15093312011494558416"},"user_tz":-330},"id":"_c-IoBURe5Z4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1704364289897,"user":{"displayName":"RASIL RISAM F 21CSR163","userId":"15093312011494558416"},"user_tz":-330},"id":"E4n3apSDfe5h"},"outputs":[],"source":["# Define a simple CNN model\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.relu3 = nn.ReLU()\n","        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n","        self.relu4 = nn.ReLU()\n","        self.fc2 = nn.Linear(128, 10)  # 10 classes in CIFAR-10\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu1(x)\n","        x = self.maxpool1(x)\n","\n","        x = self.conv2(x)\n","        x = self.relu2(x)\n","        x = self.maxpool2(x)\n","\n","        x = self.conv3(x)\n","        x = self.relu3(x)\n","        x = self.maxpool3(x)\n","\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu4(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7783,"status":"ok","timestamp":1704364297656,"user":{"displayName":"RASIL RISAM F 21CSR163","userId":"15093312011494558416"},"user_tz":-330},"id":"BOVG6YyLfkF4","outputId":"b3e65308-7302-4ad9-9b53-8bcc6c4891ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:04\u003c00:00, 42332145.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n"]}],"source":["# Load CIFAR-10 dataset and apply transformations\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":125,"status":"ok","timestamp":1704364297658,"user":{"displayName":"RASIL RISAM F 21CSR163","userId":"15093312011494558416"},"user_tz":-330},"id":"tY-xrEmafpm5"},"outputs":[],"source":["# Initialize the model, loss function, and optimizer\n","cnn_model = SimpleCNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(cnn_model.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"7MlNX0FAfu_v"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,  2000] loss: 2.124\n","[1,  4000] loss: 1.750\n","[1,  6000] loss: 1.559\n","[1,  8000] loss: 1.462\n","[1, 10000] loss: 1.339\n","[1, 12000] loss: 1.278\n","[2,  2000] loss: 1.164\n","[2,  4000] loss: 1.122\n","[2,  6000] loss: 1.104\n","[2,  8000] loss: 1.070\n","[2, 10000] loss: 1.035\n","[2, 12000] loss: 1.001\n","[3,  2000] loss: 0.917\n","[3,  4000] loss: 0.903\n","[3,  6000] loss: 0.893\n","[3,  8000] loss: 0.882\n","[3, 10000] loss: 0.876\n","[3, 12000] loss: 0.886\n","[4,  2000] loss: 0.765\n","[4,  4000] loss: 0.770\n","[4,  6000] loss: 0.778\n","[4,  8000] loss: 0.787\n","[4, 10000] loss: 0.784\n","[4, 12000] loss: 0.753\n","[5,  2000] loss: 0.669\n","[5,  4000] loss: 0.669\n","[5,  6000] loss: 0.678\n","[5,  8000] loss: 0.689\n","[5, 10000] loss: 0.691\n","[5, 12000] loss: 0.706\n","Finished Training\n"]}],"source":["# Training the model\n","for epoch in range(5):  # You can increase the number of epochs\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","\n","        optimizer.zero_grad()\n","\n","        outputs = cnn_model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:  # Print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DxbEasLhjm0s"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","provenance":[{"file_id":"1cFxz0nER_7Vbgaacn0I8hQGqo_9ReKBP","timestamp":1704364092034}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}